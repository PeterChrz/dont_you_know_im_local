# dont_you_know_im_local
Creating containers running ollama and hosting local llm. 
