FROM docker.io/rocm/vllm-dev:nightly

# Set working directory
WORKDIR /app

# Expose vLLM API port
EXPOSE 11434

# Start vLLM OpenAI-compatible server
# Model will be downloaded on first run and cached in the volume
CMD ["python", "-m", "vllm.entrypoints.openai.api_server", \
     "--model", "ibm-granite/granite-3b-code-base-2k", \
     "--host", "0.0.0.0", \
     "--port", "11434", \
     "--max-model-len", "2048", \
     "--enforce-eager"]
